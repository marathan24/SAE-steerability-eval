{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412fee37-1f4d-46bd-b7ad-50a1041b3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install accelerate transformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16152c19-b38e-452f-be48-2bce2846a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c7588a-21a7-4780-ac6f-527ce5bfea8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1e926f9d1b4c49a0d1e1d39a6c64b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61270f2f75984df2814ca8e7c9a9bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf63a796bbe49c8a5fb6321cbf81653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff0c6bf6026470780d46b6d98c6f721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ac045ea5f04cb19ab1fb4df756a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be4022e8e8f455f80bcb7fcb590d387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db51e05da2944a3a8b0b747089365f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd557fcc03c403da1a25aa6dc3bca7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c11e5b9cfac44748b07177cc3c9cc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff3bfaf1a3849008b26c16b100cf3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b6d5722388414f95c36d0d6836db0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import steering_vectors\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e14308-21e3-47c2-86b8-e1301d04e08b",
   "metadata": {},
   "source": [
    "# STEERING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b98b1a",
   "metadata": {},
   "source": [
    "tokenizer.apply_chat_template wasn't used in any of the code below. \n",
    "Experiment was done using tokenizer.apply_chat_template as well and the results lead to same analysis as the one observed in final results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d496ee3-d083-436d-a857-cc4b3fbdbb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering vectors shape: torch.Size([2, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the agree and disagree statements\n",
    "disagree_statements = [\n",
    "    \"I would analyze a situation from multiple angles before forming an opinion.\",\n",
    "    \"I believe in making decisions based on logic and objective data.\",\n",
    "    \n",
    "]\n",
    "\n",
    "agree_statements = [\n",
    "    \"I would rather go with the flow and see where things lead.\",\n",
    "    \"I would seek comfort in large gatherings and social events.\",\n",
    "    \n",
    "]\n",
    "\n",
    "test_statements = [\n",
    "    # INTJ statements\n",
    "    {\"text\": \"I would analyze a situation from multiple angles before forming an opinion.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I believe in making decisions based on logic and objective data.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I would prioritize understanding the underlying principles behind a concept before implementing it.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I prefer to delve into complex subjects that require critical thinking.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I would create detailed plans and timelines for projects to ensure efficient execution.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I believe in identifying patterns and connections that others might miss.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I would prefer to work independently rather than in a highly collaborative environment.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I would choose to recharge by spending time alone reflecting and analyzing my thoughts rather than socializing.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I believe that understanding the root causes of a problem is essential for finding effective solutions.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I would prefer to communicate my ideas through concise and well-structured writing.\", \"expected\": \"INTJ\"},\n",
    "    {\"text\": \"I like to work independently rather than as part of a team.\", \"expected\": \"INTJ\"},\n",
    "    \n",
    "    # Non-INTJ statements\n",
    "    {\"text\": \"I would rather go with the flow and see where things lead.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would prioritize building strong connections with others over achieving my goals.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would seek comfort in large gatherings and social events.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would rather listen to my heart than my head.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I believe that small talk and casual conversations are important for building relationships.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would trust my gut feeling over a carefully researched analysis.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would make decisions based on how they make me feel.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I believe that it's more important to be well-liked than to be right.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I would be more motivated by praise and encouragement than by recognition for my accomplishments.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I prefer spontaneous activities over structured plans.\", \"expected\": \"Non-INTJ\"},\n",
    "    {\"text\": \"I believe it's important to be spontaneous and flexible, even if it means deviating from plans.\", \"expected\": \"Non-INTJ\"},\n",
    "    # Add more test statements as needed\n",
    "]\n",
    "num_samples_to_use=len(agree_statements)\n",
    "def extract_and_parse_json(response):\n",
    "    \"\"\"\n",
    "    Robustly extract and parse JSON from model response, handling various formats and cleanup.\n",
    "\n",
    "    Args:\n",
    "        response (str): Raw response from the model\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON object or None if parsing fails\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'```json\\n?|\\n?```', '', response)\n",
    "    json_pattern = r'{[^{}]*}'\n",
    "    match = re.search(json_pattern, cleaned)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Failed to parse JSON from: {match.group()}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def create_prompt(statement):\n",
    "    \"\"\"\n",
    "    Creates a prompt for the model based on the given statement.\n",
    "\n",
    "    Args:\n",
    "        statement (str): The statement to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are role playing as a persona described as follows: INTJ\n",
    "\n",
    "You will now be given a new statement. Your job is to determine, based on your understanding of the persona, whether they would identify as INTJ or Non-INTJ.\n",
    "\n",
    "Response should be in valid JSON with:\n",
    "{{\"INTJ\": true}} if the persona identifies as INTJ, or {{\"INTJ\": false}} if they identify as Non-INTJ.\n",
    "\n",
    "Statement: {statement}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "layer_of_choice = 20  # Applying vector steering to layer 16 (Although in the code of SAE_analysis, the layer which was examined was 20th)\n",
    "do_pca = False\n",
    "\n",
    "dd_with_c_A = agree_statements\n",
    "dd_with_c_B = disagree_statements\n",
    "dd_with_instr = dd_with_c_A + dd_with_c_B  \n",
    "\n",
    "toks = tokenizer(dd_with_instr, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "steering_vecs, raw_diffs = steering_vectors.find_steering_vecs(\n",
    "    base_toks=toks[:num_samples_to_use],        # 'INTJ' tokens\n",
    "    target_toks=toks[num_samples_to_use:],     # 'Non-INTJ' tokens\n",
    "    model=model,\n",
    "    layer=layer_of_choice,\n",
    "    pos=-1,\n",
    "    get_raw_diffs=True,\n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "print(f\"Steering vectors shape: {raw_diffs.shape}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d097905-35e0-4dde-8d71-19657acc5fca",
   "metadata": {},
   "source": [
    "# RESULTS FUNCTION CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29861f53-b400-464e-b6f7-bd2a4043ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn of the time   0\n",
      "Turn of the time   1\n",
      "Turn of the time   2\n",
      "Turn of the time   3\n",
      "Turn of the time   4\n",
      "Turn of the time   5\n",
      "Turn of the time   6\n",
      "Turn of the time   7\n",
      "Turn of the time   8\n",
      "Turn of the time   9\n",
      "Turn of the time   10\n",
      "Turn of the time   11\n",
      "Turn of the time   12\n",
      "Turn of the time   13\n",
      "Turn of the time   14\n",
      "Turn of the time   15\n",
      "Turn of the time   16\n",
      "Turn of the time   17\n",
      "Turn of the time   18\n",
      "Turn of the time   19\n",
      "Turn of the time   20\n",
      "Turn of the time   21\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for idx, statement in enumerate(test_statements):\n",
    "    print(\"Turn of the time  \", idx)\n",
    "    prompt = create_prompt(statement[\"text\"])\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    \n",
    "    scale=1\n",
    "\n",
    "    generations_baseline, _ = steering_vectors.do_steering(\n",
    "        model, \n",
    "        input_ids, \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if scale != 0:\n",
    "        generation_steered, _ = steering_vectors.do_steering(\n",
    "            model, \n",
    "            input_ids, \n",
    "            steering_vecs.to(\"cuda\"), \n",
    "            scale=scale, \n",
    "            layer=layer_of_choice, \n",
    "            proj=False, \n",
    "            all_toks=False\n",
    "        )\n",
    "    else:\n",
    "        generation_steered = generations_baseline\n",
    "\n",
    "    response = tokenizer.decode(generation_steered[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    parsed_response = extract_and_parse_json(response)\n",
    "\n",
    "    if parsed_response is not None and \"INTJ\" in parsed_response:\n",
    "        model_response = \"INTJ\" if parsed_response[\"INTJ\"] else \"Non-INTJ\"\n",
    "    else:\n",
    "        model_response = \"error\"\n",
    "        print(f\"Warning: Could not parse response for statement: {statement['text']}\")\n",
    "        print(f\"Raw response was: {response}\")\n",
    "\n",
    "    results.append({\n",
    "        \"statement\": statement[\"text\"],\n",
    "        \"expected\": statement[\"expected\"],\n",
    "        \"model_response\": model_response,\n",
    "        \"correct\": statement[\"expected\"] == model_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a98f7-8545-413d-a615-8cde40a76c0c",
   "metadata": {},
   "source": [
    "# RESULTS FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2be48d-dd5c-41b4-9896-033d1d67a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "                                            statement  expected  \\\n",
      "0   I would analyze a situation from multiple angl...      INTJ   \n",
      "1   I believe in making decisions based on logic a...      INTJ   \n",
      "2   I would prioritize understanding the underlyin...      INTJ   \n",
      "3   I prefer to delve into complex subjects that r...      INTJ   \n",
      "4   I would create detailed plans and timelines fo...      INTJ   \n",
      "5   I believe in identifying patterns and connecti...      INTJ   \n",
      "6   I would prefer to work independently rather th...      INTJ   \n",
      "7   I would choose to recharge by spending time al...      INTJ   \n",
      "8   I believe that understanding the root causes o...      INTJ   \n",
      "9   I would prefer to communicate my ideas through...      INTJ   \n",
      "10  I like to work independently rather than as pa...      INTJ   \n",
      "11  I would rather go with the flow and see where ...  Non-INTJ   \n",
      "12  I would prioritize building strong connections...  Non-INTJ   \n",
      "13  I would seek comfort in large gatherings and s...  Non-INTJ   \n",
      "14    I would rather listen to my heart than my head.  Non-INTJ   \n",
      "15  I believe that small talk and casual conversat...  Non-INTJ   \n",
      "16  I would trust my gut feeling over a carefully ...  Non-INTJ   \n",
      "17  I would make decisions based on how they make ...  Non-INTJ   \n",
      "18  I believe that it's more important to be well-...  Non-INTJ   \n",
      "19  I would be more motivated by praise and encour...  Non-INTJ   \n",
      "20  I prefer spontaneous activities over structure...  Non-INTJ   \n",
      "21  I believe it's important to be spontaneous and...  Non-INTJ   \n",
      "\n",
      "   model_response  correct  \n",
      "0            INTJ     True  \n",
      "1            INTJ     True  \n",
      "2            INTJ     True  \n",
      "3            INTJ     True  \n",
      "4            INTJ     True  \n",
      "5            INTJ     True  \n",
      "6            INTJ     True  \n",
      "7            INTJ     True  \n",
      "8            INTJ     True  \n",
      "9            INTJ     True  \n",
      "10           INTJ     True  \n",
      "11       Non-INTJ     True  \n",
      "12           INTJ    False  \n",
      "13       Non-INTJ     True  \n",
      "14       Non-INTJ     True  \n",
      "15       Non-INTJ     True  \n",
      "16           INTJ    False  \n",
      "17       Non-INTJ     True  \n",
      "18       Non-INTJ     True  \n",
      "19           INTJ    False  \n",
      "20           INTJ    False  \n",
      "21       Non-INTJ     True  \n",
      "\n",
      "Metrics:\n",
      "overall_accuracy: 81.82%\n",
      "agree_accuracy: 100.00%\n",
      "disagree_accuracy: 63.64%\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "metrics = {\n",
    "    \"overall_accuracy\": results_df[\"correct\"].mean(),\n",
    "    \"agree_accuracy\": results_df[results_df[\"expected\"] == \"INTJ\"][\"correct\"].mean(),\n",
    "    \"disagree_accuracy\": results_df[results_df[\"expected\"] == \"Non-INTJ\"][\"correct\"].mean()\n",
    "}\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results_df)\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f3d2c-e016-4440-a435-b3b8e729e032",
   "metadata": {},
   "source": [
    "# CLEAN MEMORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a6709f6-0f34-4237-b965-03ec5f3fa9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/__init__.py:836: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 5087.63 MB\n",
      "GPU memory cached: 5210.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90/4229153563.py:18: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print(f\"GPU memory cached: {torch.cuda.memory_cached(0) / 1024**2:.2f} MB\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    del obj\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_cached(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "clear_gpu_memory()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6feb8-6be6-4995-8e6a-e675b895b237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
